# 카프카 컨슈머

***토픽 파티션에서 레코드 조회 코드***
```java
Properties prop = new Properties();
prop.put("bootstrap.servers", "localhost:9092");
prop.put("group.id", "group1");
prop.put("key.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");
prop.put("value.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");

KafkaConsumer<String, String> consumer = new KafkaConsumer<String, String>(prop);
consumer.subscribe(Collections.singleton("simple")); // 토픽구독
while(condition) {
    ConsumerRecords<String, String> records = consumer.poll(Duration.ofMills(100));
    for(ConsumerRecord<String, String> record : records) {
        System.out.println(record.value() + ":" + record.topic() + ":" + record.partition() + ":" + record.offset());
    }
}
consumer.close();
```
---

## 토픽 파티션은 그룹 단위 할당
- 컨슈머 그룹 단위로 파티션이 할당된다.

토픽의 파티션 수 >= 컨슈머 그룹내 컨슈머 수 

---

## 커밋과 오프셋
컨슈머의 `poll`메소드는 이전 커밋 오프셋의 다음 오프셋 이후부터 레코드를 읽고 읽은 레코드의 오프셋을 커밋한다.

---

## 커밋된 오프셋이 없는 경우
처음 접근하거나 커밋한 오프셋이 없는 경우 `auto.offset.reset`설정에 따라 동작 이 아래와 같이 달라진다.

`auto.offset.reset`설정값
- earliest: 맨 처음 오프셋 사용
- lastest: 가장 마지막 오프셋 사용 (기본값)
- none: 컨슈머 그룹에 대한 이전 커밋이 없으면 익셉션 발생

---

## 컨슈머 설정
조회에 영향을 주는 설정 
- `fetch.min.bytes`: 조회시 브로커가 전송할 최소 데이터 크기
    - 기본값: 1
    - 이 값이 크면 대기 시간은 늘지만 처리량이 증가
- `fetch.max.wait.ms`: 데이터가 최소 크기가 될 때까지 기다릴 시간
    - 기본값: 500
    - 브로커가 리턴할 때까지 대기하는 시간으로 poll() 메서드의 대기 시간과 다름
- `max.partition.fetch.bytes`: 파티션 당 서버가 리턴할 수 있는 최대 크기
    - 기본값: 1048576 (1MB)    

---

## 자동 커밋/ 수동 커밋
- `enable.auto.commit`설정
    - true: 일정 주기로 컨슈머가 읽은 오프셋을 커밋(기본값)
    - false: 수동으로 커밋실행
- `auto.commit.interval.ms`- 자동 커밋 주기
    - 기본값: 5000ms(5초)
- poll(), close() 메서드 호출시 자동 커밋 실행

---

## 수동 커밋 : 동기/비동기 커밋
***동기 커밋***
```java
ConsumerRecords<String, String> records = consumer.poll(Duration.ofSeconds(1));
for(ConsumerRecord<String, String> record : recorcs) {
    // ... 비즈니스 로직 (레코드 처리)
}

try {
    consumer.commitSync();
} catch(Exception ex) {
    // 커밋 실패시 에러 발생
}
```
***비동기 커밋***
```java
ConsumerRecords<String, String> records = consumer.poll(Duration.ofSeconds(1));
for(ConsumerRecord<String, String> record : recorcs) {
    // ... 비즈니스 로직 (레코드 처리)
}
consumer.commitAsync(); // commitAsync(OffsetCommitCallback callback)
```

---

## 재처리와 순서
- 동일 메시지 조회 가능성
    - 일시적 커밋 실패, 리밸런스 등에 의해 발생
- 컨슈머는 멱등성(idempotence)을 고려해야 함
    - ex: 아래 메시지를 재처리 할 경우
        - 조회수 1증가 -> 좋아요 1증가 -> 조회수 1증가
    - 단순 처리하면 조회수는 2가 아닌 4가 될 수 있음
- 데이터 특성에 따라 타임스탬프, 일련 번호 등을 활용

---

## 세션 타임아웃, 하트비트, 최대 poll 간격
- 컨슈머는 하트비트를 전송해서 연결 유지
    - 브로커는 일정 시간 컨슈머로부터 하트비트가 없으면 컨슈머를 그룹에서 빼고 리밸런스 진행
    - 관련설정
        - `session.timeout.ms`: 세션 타임 아웃 시간(기본값 10초)
        - `heartbeat.interval.ms`: 하트 비트의 전송주기(기본값 3초)
            -> session.timeout.ms의 1/3 이하 추천

- `max.poll.interval.ms`: poll() 메서드의 최대 호출 간격
    - 이 시간이 지나도록 poll()하지 않으면 컨슈머를 그룹에서 빼고 리밸런스 진행

---

## 종료 처리
- 다른 쓰레드에서 wakeup() 메서드 호출
    - poll()메서드가 WakeupException 발생 -> close() 메서드로 종료처리
```java
KafkaConsumer<String, String> consumer = new KafkaConsumer<String, String>(prop);
consumer.subscribe(Collections.singleton("simple")); // 토픽구독
try{
    while(condition) {
    ConsumerRecords<String, String> records = consumer.poll(Duration.ofMills
    (100)); // -> wakeup() 호출시 익셉션 발생
    // ... records 처리
    try {
        consumer.commitAsync();
    } catch (Exception e) {
        e.printStackTrace();
    }
}
} catch (Exception ex) {
    ...
} finally {
    consumer.close();
}
```
---

## 주의: 쓰레드에 안전하지 않음
- KafkaConsumer는 쓰레드에 안전하지 않음
    - 여러 쓰레드에서 동시에 사용해서는 안된다.
    - wakeup() 메서드는 예외이다.